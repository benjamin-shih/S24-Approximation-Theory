\section{Week 3: 3/7/2024}
\subsection{Convergence for Differentiable Functions}
Generally, we accept the following to be a motif in approximation:
\begin{quote}
  The smoother a function, the faster its approximants converge as $n \rightarrow \infty$. 
\end{quote}
Generally, if a function $f \in C^k([-1, 1])$, then convergence of its Chebyshev interpolants occurs on the rate of $O(n^{-k})$ with respect to the supremum norm. Note that if $f$ is not \emph{continuosuly} differentiable, then may not be true. However, if $f$ has \emph{bounded variation} ($f$ has bounded variation if its total variation---the $1$-norm of the derivative---is finite), then it enjoys this convergence. 
\begin{dfn}[Absolute continuity (heuristic)]
    A function $f$ is absolutely continuous if it is equal to the integral of its derivative, which exists almost everywhere and is Lebesgue integrable. 
\end{dfn}
With this definition, we can establish a bound on the Chebyshev coefficients for differentiable functions.
\begin{theorem}[Chebyshev coefficients for differentiable functions]
  For an integer $\nu \geq 0$, let $f$ and its derivatives thorugh $f^{\nu - 1)}$ be absolutely continuous on $[-1, 1]$ and suppose the $\nu$-th derivative $f^\nu$ is of bounded variation $V$. Then for $k \geq \nu + 1$, the Chebyshev coefficients of $f$ satisfy
  \[ |a_k| \leq \frac{2V}{\pi k(k -1)\cdot(k - \nu)} \leq \frac{2V}{\pi(k-\nu)^{\nu + 1}}\]
  \label{thm: cheby-coeff}
\end{theorem}
With this bound on the coefficients, we establish the following convergence result:
\begin{theorem}[Convergence for differentiable functions]
  If $f$ satisfies the conditions of Theorem \ref{thm: cheby-coeff}, then with $V$ denoting the total variation of $f^{(\nu)}$ for some $\nu \geq 1$, then for any $n > \nu$, its Chebyshev projections satisfy
  \[ \|f - f_n\| \leq \frac{2V}{\pi \nu (n-\nu)^\nu} \]
  and its Chebyshev interpolants satisfy
  \[ \| f - p_n\| \leq \frac{4V}{\pi\nu(n-\nu)^\nu}\]
\end{theorem}

\subsection{Gibbs Phenomenon}
We now study the \emph{Gibbs Phenomenon}, which describes how polynomial interpolants and projections oscillate and overshoot near discontinuities. Note that as $n \rightarrow \infty$, the overshoot gets narrower, but not shorter!
\begin{theorem}[Gibbs Phenomenon for Chebyshev interpolants]
  Let $p_n$ be the degree $n$ Chebyshev interpolant of the function $f(x) = \operatorname{sign}(x)$ on $[-1, 1]$. Then as $n \rightarrow \infty$,
  \begin{align*}
    \lim_{n \rightarrow \infty, n \text{ odd}} \|p_n \| &= c_1 \approx 1.282 \\ 
    \lim_{n \rightarrow \infty, n \text{ even}} \|p_n \| &= c_2 \approx 1.066
  \end{align*}
  These values are the height or maximum attained by the Chebyshev interpolant on $f(x)$.
\end{theorem}

\subsection{Density of Two Layer Neural Networks}
We consider the family of two-layer feedforward networks consisting of:
\begin{itemize}
  \item $d$ input neurons 
  \item $r$ neurons in one hidden layer using the same activation $\sigma: \R \rightarrow \R$
  \item one output nueron with no activation nor bias.
\end{itemize}
\begin{figure}[bth]
  \centering
	\begin{tikzpicture}[x=2.2cm,y=1.4cm]
		\readlist\Nnod{2,3,1} % array of number of nodes per layer
		\message{^^J  Layer}
		\foreachitem \N \in \Nnod{ % loop over layers
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\message{\lay,}
			\foreach \i [evaluate={\y=\N/2-\i; \x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
					% NODES
					\node[node \n] (N\lay-\i) at (\x,\y) {};
					% CONNECTIONS
					\ifnum\lay>1 % connect to previous layer
						\foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
								\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
								\draw[connect] (N\prev-\j) -- (N\lay-\i);
								%\draw[connect] (N\prev-\j.0) -- (N\lay-\i.180); % connect to left
							}
					\fi % else: nothing to connect first layer
				}
		}
		% LABELS
		\node[above=5,align=center,ctpTeal!60!black] at (N1-1.90) {Input \\[-0.2em]layer};
		\node[above=2,align=center,ctpBlue!60!black] at (N2-1.90) {Hidden layer};
		\node[above=10,align=center,ctpRed!60!black] at (N\Nnodlen-1.90) {Output\\[-0.2em]layer};
	\end{tikzpicture}
  \caption{A feed-forward neural network with one hidden layer, where $d=2, r=3$.}
\end{figure}

If we let $x \in \R^d$ be the input to the network, then the ourput $y \in \R$ can be written
\begin{displaymath}
  y = \sum_{i=1}^r W_{1, i}^2 \sigma\left( \displaystyle\sum_{j=1}^{d}W_{i, j}^1 x_j + b_i\right)
\end{displaymath}
where $W^1 \in \R^{r \times d}$ and $W^2 \in \R^{1 \times r}$ are the weight matrices for the layer and $b \in \R^r$ is the bias in the hidden layer. Then the main desnity result for such networks is as follows. 
\begin{theorem}[Pinkus]
  Let
  \begin{displaymath}
    \mathcal{M}(\sigma) = \operatorname{span}\{ \sigma(w\cdot x + b): w \in \R^d, b \in \R\}
  \end{displaymath}
  where $\sigma \in C(\R)$. Then $\mathcal{M}(\sigma)$ is dense in $C(\R^n)$ with respect to the supremum norm on compact sets, if and only if $\sigma$ is not a polynomial.
\end{theorem}
Essentially, Pinkus' theorem states that the network space $\mathcal{M}(\sigma)$ is dense in the space of continuous functions with respect to the supremum norm as long as the class of activation function chosen is non-polynomial. This means that given a target function $f \in C(\R^d)$ and a compact subset $X \subset \R^d$, for any $\epsilon > 0$, there exists $g \in \mathcal{M}(\sigma)$ so that 
\[ \sup_{x \in X} | f(x) - g(x) | < \epsilon \]
Examples of such $\sigma$ which would satisfy the necessary conditions of Pinkus' theorem would be $\sin(\cdot), \cos(\cdot)$, etc. Let us consider the proof of the $1$-dimensional case of the theorem; the extension to multiple dimensions can be found in \cite{surv}.

\paragraph{Proof of the $1$-dimensional case of Pinkus' theorem} In this proof we consider the $1$-dimensional case of $\mathcal{M}(\sigma)$, which we will denote
\[ \mathcal{N}(\sigma) = \operatorname{span}\{(wx+b): w, b \in \R\} \]
We will show that for any non-polynomial $\sigma$, $\mathcal{N}(\sigma)$ is dense in $C(\R)$.
